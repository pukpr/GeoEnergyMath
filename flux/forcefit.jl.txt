using Flux, Plots, CSV, DataFrames, Distributions;

function forcefit(n_loop, n_hidden, n_layers, N1, N::Integer, file_name::String)
           ## Parameterize training NN
  function create_repeated_dense_layers(n_in, n_hidden, n_out, n_layers, activation)
     layers = []
     push!(layers, Dense(n_in, n_hidden, activation))
     for _ in 2:n_layers-1
        push!(layers, Dense(n_hidden, n_hidden, activation))
     end
     push!(layers, Dense(n_hidden, n_out))
     return layers
  end
           ## Set up training data
  df=CSV.read(file_name, DataFrame; delim='\t')
  N0 = nrow(df);
  t=Vector{Float32}(df[1:N0,1]) # 1 Time
  m=Vector{Float32}(df[1:N0,2]) # 2 Model (LTE)
  d=Vector{Float32}(df[1:N0,3]) # 3 Data
  f=Vector{Float32}(df[1:N0,4]) # 4 Forcing (LTE)
           ## Set up training model
  model = Chain(create_repeated_dense_layers(1, n_hidden, 1, n_layers, tanh))
  loss(input, output) = Flux.Losses.mse(model(input), output)
  optimiser = Descent(0.1);
  train_data = [(Array(f[N1:N]'), Array(d[N1:N]'))];
           ## Train model and periodically evaluate
  for Epoch in 1:n_loop
    Flux.train!(loss, Flux.params(model), train_data, optimiser)
    if Epoch % 1000 == 1
       m = map(x -> model([x])[1], f)
       println([cor(d[N1:N],m[N1:N]),  cor(d[N:N0],m[N:N0])])
       plot(t, d, ylims=(-4,4), lw=0.5, label="Data", xlabel="Year", ylabel="index value", title=file_name)  
       plot!(t[1:N0], m[1:N0],  lw=0.5, label="Predicted")  # smoothed=true
       plot!(t[N1:N], m[N1:N],  lw=0.5, label="Trained"); display(plot!())
    end
  end
end;
